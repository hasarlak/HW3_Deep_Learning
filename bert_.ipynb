{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/HW3DL/D3')\n",
    "\n",
    "\n",
    "! ls\n",
    "!pip install packaging==21.3\n",
    "!pip install transformers==4.5.0\n",
    "\n",
    "from transformers import AdamW, BertTokenizerFast, BertForQuestionAnswering\n",
    "\n",
    "# model_name can be one of models in huggingface model hub \n",
    "\n",
    "model_name = 'bert-base-chinese'\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "eng_tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "eng_paragraph = 'professor feng Deep Learning class'\n",
    "\n",
    "tokens = eng_tokenizer.tokenize(eng_paragraph)\n",
    "print(tokens)\n",
    "eng_tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "question = 'Who established Apple?'\n",
    "paragraph = 'Apple Computers, Inc. was founded on April 1, 1976, by college dropouts Steve Jobs and Steve Wozniak'\n",
    "\n",
    "encoded = eng_tokenizer.encode(question, paragraph)\n",
    "decoded = eng_tokenizer.decode(encoded)\n",
    "print(encoded)\n",
    "print(decoded)\n",
    "\n",
    "inputs = eng_tokenizer(question, paragraph, return_tensors='pt') \n",
    "\n",
    "print('##################')\n",
    "# Indices of input sequence tokens in the vocabulary\n",
    "print('Input ids:      ', inputs['input_ids'])\n",
    "print('##################')\n",
    "# Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n",
    "print('Token type ids: ', inputs['token_type_ids'])\n",
    "print('##################')\n",
    "# Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
    "print('Attention mask: ', inputs['attention_mask'])\n",
    "\n",
    "output = model(**inputs, start_positions=torch.tensor([14]), end_positions=torch.tensor([19]))\n",
    "print(\"loss: \", output.loss)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "output.loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "question = 'when  Apple was founded?'\n",
    "paragraph = 'Apple Computers, Inc. was founded on April 1, 1976, by college dropouts Steve Jobs and Steve Wozniak'\n",
    "\n",
    "inputs = eng_tokenizer(question, paragraph, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(**inputs)\n",
    "\n",
    "print(\"start_logits: \")\n",
    "print(output.start_logits)\n",
    "\n",
    "print(\"end_logits: \")\n",
    "print(output.end_logits)\n",
    "\n",
    "start = torch.argmax(output.start_logits)\n",
    "end = torch.argmax(output.end_logits)\n",
    "print(\"start position: \", start.item())\n",
    "print(\"end position:   \", end.item())\n",
    "\n",
    "predict_id = inputs['input_ids'][0][start : end + 1]\n",
    "print(\"predict_id:     \", predict_id)\n",
    "\n",
    "predict_answer = eng_tokenizer.decode(predict_id)\n",
    "print(\"predict_answer: \", predict_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125aabbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from transformers import AdamW, BertForQuestionAnswering, BertTokenizerFast\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\", 1) if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(0)    # Fix random seed for reproducibility\n",
    "\n",
    "# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\n",
    "\n",
    "fp16_training = True\n",
    "\n",
    "if fp16_training:\n",
    "    %pip install accelerate==0.2.0\n",
    "    from accelerate import Accelerator\n",
    "    accelerator = Accelerator(fp16=True)\n",
    "    device = accelerator.device\n",
    "    \n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-chinese\").to(device)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "\n",
    "def read_data(file):\n",
    "    with open(file, 'r', encoding=\"utf-8\") as reader:\n",
    "        data = json.load(reader)\n",
    "    return data[\"questions\"], data[\"paragraphs\"]\n",
    "\n",
    "train_questions, train_paragraphs = read_data(\"train.json\")\n",
    "dev_questions, dev_paragraphs     = read_data(\"dev.json\")\n",
    "test_questions, test_paragraphs   = read_data(\"test.json\")\n",
    "\n",
    "print('train_questions : ', len(train_questions))\n",
    "print('dev_questions   : ', len(dev_questions))\n",
    "print('test_questions  : ', len(test_questions))\n",
    "\n",
    "# Tokenize questions and paragraphs separately\n",
    "\n",
    "train_questions_tokenized  = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\n",
    "dev_questions_tokenized    = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\n",
    "test_questions_tokenized   = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False) \n",
    "\n",
    "train_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False)\n",
    "dev_paragraphs_tokenized   = tokenizer(dev_paragraphs, add_special_tokens=False)\n",
    "test_paragraphs_tokenized  = tokenizer(test_paragraphs, add_special_tokens=False)\n",
    "\n",
    "DOC_STRIDE = None\n",
    "\n",
    "class QA_Dataset(Dataset):\n",
    "    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n",
    "        self.split = split\n",
    "        self.questions = questions\n",
    "        self.tokenized_questions = tokenized_questions\n",
    "        self.tokenized_paragraphs = tokenized_paragraphs\n",
    "        self.max_question_len = 40\n",
    "        self.max_paragraph_len = 350\n",
    "        \n",
    "        ##### Change value of doc_stride #####\n",
    "        self.doc_stride = int(0.9 * self.max_paragraph_len)\n",
    "\n",
    "        ############################################\n",
    "        global DOC_STRIDE\n",
    "        \n",
    "        DOC_STRIDE = self.doc_stride\n",
    "        ############################################\n",
    "        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n",
    "\n",
    "        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        tokenized_question = self.tokenized_questions[idx]\n",
    "        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n",
    "\n",
    "        ##### Preprocessing #####\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n",
    "            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n",
    "            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n",
    "\n",
    "            # A single window is obtained by slicing the portion of paragraph containing the answer\n",
    "            mid = (answer_start_token + answer_end_token) // 2\n",
    "            prefix_len = int(random.random() * self.max_paragraph_len)\n",
    "            postfix_len = self.max_paragraph_len - prefix_len\n",
    "            paragraph_start, paragraph_end = mid - prefix_len, mid + postfix_len\n",
    "            if paragraph_start < 0:\n",
    "                paragraph_end -= paragraph_start\n",
    "                paragraph_start = 0\n",
    "            if paragraph_end >= len(tokenized_paragraph):\n",
    "                paragraph_end = len(tokenized_paragraph) - 1\n",
    "            \n",
    "            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] \n",
    "            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\n",
    "            \n",
    "            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  \n",
    "            answer_start_token += len(input_ids_question) - paragraph_start\n",
    "            answer_end_token += len(input_ids_question) - paragraph_start\n",
    "            \n",
    "            # Pad sequence and obtain inputs to model \n",
    "            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n",
    "\n",
    "        # Validation\n",
    "        # Testing\n",
    "        else:\n",
    "            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n",
    "            \n",
    "            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n",
    "            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n",
    "                \n",
    "                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
    "                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n",
    "                \n",
    "                # Pad sequence and obtain inputs to model\n",
    "                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "                \n",
    "                input_ids_list.append(input_ids)\n",
    "                token_type_ids_list.append(token_type_ids)\n",
    "                attention_mask_list.append(attention_mask)\n",
    "            \n",
    "            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n",
    "\n",
    "    def padding(self, input_ids_question, input_ids_paragraph):\n",
    "        # Pad zeros if sequence length is shorter than max_seq_len\n",
    "        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n",
    "        # Indices of input sequence tokens in the vocabulary\n",
    "        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n",
    "        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n",
    "        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n",
    "        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
    "        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask\n",
    "\n",
    "train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n",
    "dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n",
    "test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)\n",
    "\n",
    "train_batch_size = 4\n",
    "\n",
    "#########################\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
    "dev_loader   = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "test_loader  = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, output, paragraph, paragraph_tokenized):\n",
    "    ##### Postprocessing #####\n",
    "    \n",
    "    answer = ''\n",
    "    max_prob = float('-inf')\n",
    "    num_of_windows = data[0].shape[1]\n",
    "    \n",
    "    paragraph_start_index = 0\n",
    "    paragraph_end_index = 0\n",
    "    \n",
    "    for k in range(num_of_windows):\n",
    "\n",
    "        mask = (data[1][0][k].bool() & data[2][0][k].bool()).to(device)\n",
    "    \n",
    "        masked_output_start = torch.masked_select(output.start_logits[k], mask)\n",
    "        masked_output_start = masked_output_start[:-1]\n",
    "        \n",
    "        start_prob, start_index = torch.max(masked_output_start, dim=0)\n",
    "        \n",
    "        masked_output_end = torch.masked_select(output.end_logits[k], mask)\n",
    "        masked_output_end = masked_output_end[start_index: -1]\n",
    "        \n",
    "        end_prob, end_index = torch.max(masked_output_end, dim=0)\n",
    "        \n",
    "        end_index += start_index\n",
    "        \n",
    "        # Probability of answer is calculated as sum of start_prob and end_prob\n",
    "        prob = start_prob + end_prob\n",
    "        masked_data = torch.masked_select(data[0][0][k].to(device), mask)[:-1]\n",
    "        \n",
    "        # Replace answer if calculated probability is larger than previous windows\n",
    "        if (prob > max_prob) and (start_index <= end_index <= (start_index + 50)):\n",
    "            max_prob = prob\n",
    "            paragraph_start_index = start_index.item() + (DOC_STRIDE * k)\n",
    "            paragraph_end_index = end_index.item() + (DOC_STRIDE * k)\n",
    "            answer = tokenizer.decode(masked_data[start_index : end_index + 1])\n",
    "\n",
    "\n",
    "    \n",
    "    ##########\n",
    "    char_count = 0\n",
    "    start_flag = False\n",
    "\n",
    "    for i, token in enumerate(paragraph_tokenized):\n",
    "        if token in ('[UNK]', '[CLS]', '[SEP]'):\n",
    "            if i == paragraph_start_index:\n",
    "                new_start = char_count\n",
    "            if i == paragraph_end_index:\n",
    "                new_end = char_count\n",
    "            char_count += 1\n",
    "        else:\n",
    "            for char in token:\n",
    "                if i == paragraph_start_index and not start_flag:\n",
    "                    new_start = char_count\n",
    "                    start_flag = True\n",
    "                if i == paragraph_end_index:\n",
    "                    new_end = char_count\n",
    "                if char == \"#\":\n",
    "                    continue\n",
    "                else:\n",
    "                    while char_count < len(paragraph) and char != paragraph[char_count]:\n",
    "                        char_count += 1\n",
    "                    char_count += 1\n",
    "     \n",
    "    if \"[UNK]\" in answer:\n",
    "        print(f\"original answer: {answer}\")\n",
    "        answer = paragraph[new_start: new_end+1]\n",
    "        print(f\"corrected answer: {answer}\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "###########################################################\n",
    "\n",
    "    answer = answer.replace(' ', '')\n",
    "    \n",
    "###########################################################\n",
    "\n",
    "    if len(answer) > 1:\n",
    "        if \"「\" not in answer and answer[-1] == \"」\":\n",
    "            answer = answer[:-1]\n",
    "    return answer\n",
    "\n",
    "num_epoch     = 5  \n",
    "validation    = True  \n",
    "logging_step  = 500\n",
    "learning_rate = 5e-6\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "##### Apply linear learning rate decay #####\n",
    "total_steps = len(train_loader) * num_epoch\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "##################################################\n",
    "\n",
    "if fp16_training:\n",
    "    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader) \n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"Start Training ...\")\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    step = 1\n",
    "    train_loss = train_acc = 0\n",
    "    \n",
    "    for batch_idx, data in enumerate(tqdm(train_loader)):\n",
    "        # Load all data into GPU\n",
    "        data = [i.to(device) for i in data]\n",
    "         \n",
    "        output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n",
    "\n",
    "        # Choose the most probable start position / end position\n",
    "        start_index = torch.argmax(output.start_logits, dim=1)\n",
    "        end_index = torch.argmax(output.end_logits, dim=1)\n",
    "\n",
    "        # Prediction is correct only if both start_index and end_index are correct\n",
    "        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n",
    "        train_loss += output.loss\n",
    "\n",
    "        if fp16_training:\n",
    "            accelerator.backward(output.loss)\n",
    "        else:\n",
    "            output.loss.backward()\n",
    "\n",
    "        ##### Apply linear learning rate decay #####\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        step += 1\n",
    "        ##################################################\n",
    "        \n",
    "        # Print training loss and accuracy over past logging step\n",
    "        if step % logging_step == 0:\n",
    "            print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f}\")\n",
    "            train_loss = train_acc = 0\n",
    "            \n",
    "    if validation:\n",
    "        print(\"Evaluating Dev Set ...\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dev_acc = 0\n",
    "            for i, data in enumerate(tqdm(dev_loader)):\n",
    "                output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "                # prediction is correct only if answer text exactly matches\n",
    "                dev_acc += evaluate(data, output, dev_paragraphs[dev_questions[i]['paragraph_id']], dev_paragraphs_tokenized[dev_questions[i]['paragraph_id']].tokens) == dev_questions[i][\"answer_text\"]\n",
    "            print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n",
    "        model.train()\n",
    "\n",
    "\n",
    "##################################################\n",
    "\n",
    "# Save a model \n",
    "print(\"Saving Model ...\")\n",
    "model_save_dir = \"./models/saved_model\" \n",
    "model.save_pretrained(model_save_dir)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    dev_acc = 0\n",
    "    for i, data in enumerate(tqdm(dev_loader)):\n",
    "        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "               attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "        # prediction is correct only if answer text exactly matches\n",
    "        pred_answer = evaluate(data, output, dev_paragraphs[dev_questions[i]['paragraph_id']], dev_paragraphs_tokenized[dev_questions[i]['paragraph_id']].tokens)\n",
    "        true_answer = dev_questions[i][\"answer_text\"]\n",
    "        dev_acc += (pred_answer == true_answer)\n",
    "        if pred_answer != true_answer:\n",
    "            print(\"*\"*50)\n",
    "            print(f\"correct answer: {true_answer}\")\n",
    "            print(f\"predict answer: {pred_answer}\")\n",
    "            print(\"*\"*50)\n",
    "    print(f\"Validation | acc = {dev_acc / len(dev_loader):.3f}\")\n",
    "model.train()\n",
    "print(f\"Validation | acc = {dev_acc / len(dev_loader):.3f}\")\n",
    "\n",
    "print(\"Evaluating Test Set ...\")\n",
    "\n",
    "result = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(tqdm(test_loader)):\n",
    "        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "        result.append(evaluate(data, output, test_paragraphs[test_questions[i]['paragraph_id']], test_paragraphs_tokenized[test_questions[i]['paragraph_id']].tokens))\n",
    "\n",
    "result_file = \"./result.csv\"\n",
    "with open(result_file, 'w') as f:\n",
    "    f.write(\"ID,Answer\\n\")\n",
    "    for i, test_question in enumerate(test_questions):\n",
    "        # Replace commas in answers with empty strings (since csv is separated by comma)\n",
    "        f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n",
    "\n",
    "print(f\"Completed! Result is in {result_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25972261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_answer(answer, paragraph, paragraph_tokenized):\n",
    "    ##### Postprocessing #####\n",
    "    answer_text = ''\n",
    "    max_prob = float('-inf')\n",
    "    num_of_windows = data[0].shape[1]\n",
    "    \n",
    "    paragraph_start_index = 0\n",
    "    paragraph_end_index = 0\n",
    "    \n",
    "    for k in range(num_of_windows):\n",
    "\n",
    "        mask = (data[1][0][k].bool() & data[2][0][k].bool()).to(device_custom)\n",
    "    \n",
    "        masked_output_start = torch.masked_select(output.start_logits[k], mask)\n",
    "        masked_output_start = masked_output_start[:-1]\n",
    "        \n",
    "        start_prob, start_index = torch.max(masked_output_start, dim=0)\n",
    "        \n",
    "        masked_output_end = torch.masked_select(output.end_logits[k], mask)\n",
    "        masked_output_end = masked_output_end[start_index: -1]\n",
    "        \n",
    "        end_prob, end_index = torch.max(masked_output_end, dim=0)\n",
    "        \n",
    "        end_index += start_index\n",
    "        \n",
    "        prob = start_prob + end_prob\n",
    "        masked_data = torch.masked_select(data[0][0][k].to(device_custom), mask)[:-1]\n",
    "        \n",
    "        if (prob > max_prob) and (start_index <= end_index <= (start_index + 50)):\n",
    "            max_prob = prob\n",
    "            paragraph_start_index = start_index.item() + (DOC_STRIDE_CUSTOM * k)\n",
    "            paragraph_end_index = end_index.item() + (DOC_STRIDE_CUSTOM * k)\n",
    "            answer_text = tokenizer_custom.decode(masked_data[start_index : end_index + 1])\n",
    "\n",
    "    char_count = 0\n",
    "    start_flag = False\n",
    "\n",
    "    for i, token in enumerate(paragraph_tokenized):\n",
    "        if token in ('[UNK]', '[CLS]', '[SEP]'):\n",
    "            if i == paragraph_start_index:\n",
    "                new_start = char_count\n",
    "            if i == paragraph_end_index:\n",
    "                new_end = char_count\n",
    "            char_count += 1\n",
    "        else:\n",
    "            for char in token:\n",
    "                if i == paragraph_start_index and not start_flag:\n",
    "                    new_start = char_count\n",
    "                    start_flag = True\n",
    "                if i == paragraph_end_index:\n",
    "                    new_end = char_count\n",
    "                if char == \"#\":\n",
    "                    continue\n",
    "                else:\n",
    "                    while char_count < len(paragraph) and char != paragraph[char_count]:\n",
    "                        char_count += 1\n",
    "                    char_count += 1\n",
    "     \n",
    "    if \"[UNK]\" in answer_text:\n",
    "        print(f\"original answer: {answer_text}\")\n",
    "        answer_text = paragraph[new_start: new_end+1]\n",
    "        print(f\"corrected answer: {answer_text}\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "    answer_text = answer_text.replace(' ', '')\n",
    "    \n",
    "    if len(answer_text) > 1:\n",
    "        if \"「\" not in answer_text and answer_text[-1] == \"」\":\n",
    "            answer_text = answer_text[:-1]\n",
    "    return answer_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96875d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_custom = 5  \n",
    "validation_custom = True  \n",
    "logging_step_custom = 500\n",
    "learning_rate_custom = 5e-6\n",
    "\n",
    "optimizer_custom = AdamW(model_custom.parameters(), lr=learning_rate_custom)\n",
    "\n",
    "total_steps_custom = len(train_loader) * num_epoch_custom\n",
    "\n",
    "scheduler_custom = get_linear_schedule_with_warmup(optimizer_custom, num_warmup_steps=0, num_training_steps=total_steps_custom)\n",
    "\n",
    "if fp16_training_custom:\n",
    "    model_custom, optimizer_custom, train_loader_custom = accelerator_custom.prepare(model_custom, optimizer_custom, train_loader_custom) \n",
    "\n",
    "model_custom.train()\n",
    "\n",
    "print(\"Start Training ...\")\n",
    "\n",
    "for epoch in range(num_epoch_custom):\n",
    "    step = 1\n",
    "    train_loss = train_acc = 0\n",
    "    \n",
    "    for batch_idx, data in enumerate(tqdm(train_loader_custom)):\n",
    "        data = [i.to(device_custom) for i in data]\n",
    "        \n",
    "        output = model_custom(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n",
    "\n",
    "        start_index = torch.argmax(output.start_logits, dim=1)\n",
    "        end_index = torch.argmax(output.end_logits, dim=1)\n",
    "\n",
    "        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n",
    "        train_loss += output.loss\n",
    "\n",
    "        if fp16_training_custom:\n",
    "            accelerator_custom.backward(output.loss)\n",
    "        else:\n",
    "            output.loss.backward()\n",
    "\n",
    "        optimizer_custom.step()\n",
    "        scheduler_custom.step()\n",
    "        optimizer_custom.zero_grad()\n",
    "        step += 1\n",
    "        \n",
    "        if step % logging_step_custom == 0:\n",
    "            print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step_custom:.3f}, acc = {train_acc / logging_step_custom:.3f}\")\n",
    "            train_loss = train_acc = 0\n",
    "            \n",
    "    if validation_custom:\n",
    "        print(\"Evaluating Dev Set ...\")\n",
    "        model_custom.eval()\n",
    "        with torch.no_grad():\n",
    "            dev_acc = 0\n",
    "            for i, data in enumerate(tqdm(dev_loader_custom)):\n",
    "                output = model_custom(input_ids=data[0].squeeze(dim=0).to(device_custom), token_type_ids=data[1].squeeze(dim=0).to(device_custom),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device_custom))\n",
    "                dev_acc += evaluate(data, output, dev_paragraphs_custom[dev_questions_custom[i]['paragraph_id']], dev_paragraphs_tokenized_custom[dev_questions_custom[i]['paragraph_id']].tokens) == dev_questions_custom[i][\"answer_text\"]\n",
    "            print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader_custom):.3f}\")\n",
    "        model_custom.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Here we are saving the model\")\n",
    "model_save_dir_custom = \"./model/saved\" \n",
    "model_custom.save_pretrained(model_save_dir_custom)\n",
    "\n",
    "model_custom.eval()\n",
    "with torch.no_grad():\n",
    "    dev_acc = 0\n",
    "    for i, data in enumerate(tqdm(dev_loader_custom)):\n",
    "        output = model_custom(input_ids=data[0].squeeze(dim=0).to(device_custom), token_type_ids=data[1].squeeze(dim=0).to(device_custom),\n",
    "               attention_mask=data[2].squeeze(dim=0).to(device_custom))\n",
    "        pred_answer = evaluate(data, output, dev_paragraphs_custom[dev_questions_custom[i]['paragraph_id']], dev_paragraphs_tokenized_custom[dev_questions_custom[i]['paragraph_id']].tokens)\n",
    "        true_answer = dev_questions_custom[i][\"answer_text\"]\n",
    "        dev_acc += (pred_answer == true_answer)\n",
    "        if pred_answer != true_answer:\n",
    "            print(\"*\"*50)\n",
    "            print(f\"correct answer: {true_answer}\")\n",
    "            print(f\"predict answer: {pred_answer}\")\n",
    "            print(\"*\"*50)\n",
    "    print(f\"Validation | acc = {dev_acc / len(dev_loader_custom):.3f}\")\n",
    "model_custom.train()\n",
    "print(f\"Validation | acc = {dev_acc / len(dev_loader_custom):.3f}\")\n",
    "\n",
    "print(\"Evaluating Test Set ...\")\n",
    "\n",
    "result_custom = []\n",
    "\n",
    "model_custom.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(tqdm(test_loader_custom)):\n",
    "        output = model_custom(input_ids=data[0].squeeze(dim=0).to(device_custom), token_type_ids=data[1].squeeze(dim=0).to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247f62bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
